{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0Rxeyxzpem-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import *\n",
        "from sklearn.metrics import *\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression, LinearRegression, LogisticRegressionCV\n",
        "from sklearn.ensemble import RandomForestClassifier,  BaggingClassifier, BaggingRegressor, RandomTreesEmbedding,GradientBoostingClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "import gensim\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6gcyok3nhVd",
        "colab_type": "code",
        "outputId": "6804aa38-bb50-4e01-b46e-3d388962f024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z6AzZy5noUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise = stopwords.words('english') + list(punctuation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7IwtM6apUC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_url = 'https://raw.githubusercontent.com/TatianaShavrina/hse_ml_m1/master/ensembles/complaints.csv'\n",
        "data = pd.read_csv(data_url, sep='\\t')\n",
        "data.head()\n",
        "y = data[\"PRODUCT_ID\"]\n",
        "X = data[\"cleaned_text\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIFXCGWOpdhB",
        "colab_type": "text"
      },
      "source": [
        "### Voting Classifier и FeatureUnion + препроцессинг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jdRMwvQrAWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.preprocessing import FunctionTransformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ0XA5-Qp-nT",
        "colab_type": "code",
        "outputId": "0d0a0cf9-c8a3-450a-971f-e8faeb1e2990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "sno = nltk.stem.SnowballStemmer('english')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n81bYfgTqEkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stem_tokenizer(text):\n",
        "    return [sno.stem(t) for t in word_tokenize(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDcksJunqPJj",
        "colab_type": "code",
        "outputId": "48661f4c-1c56-4154-c2de-2c8a968581e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model = gensim.models.Word2Vec(X_train, size=100)\n",
        "w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
        "\n",
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        self.dim = 100\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2iuvfadqbiT",
        "colab_type": "code",
        "outputId": "cd4ee5f9-24cb-41a7-b497-719f5b9add00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "clf1 = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=1)\n",
        "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "clf3 = GaussianNB()\n",
        "\n",
        "eclf = VotingClassifier(estimators=[\n",
        "        ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
        "\n",
        "voting = Pipeline([\n",
        "    ('feats', FeatureUnion([\n",
        "        ('tfidf', TfidfVectorizer(ngram_range=(1,3), analyzer='word', max_features=200)), \n",
        "        ('tfidf2', TfidfVectorizer(tokenizer=stem_tokenizer)),\n",
        "        (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
        "        ('vect', CountVectorizer(tokenizer=stem_tokenizer, analyzer='word', max_features=200)),\n",
        "        ])),\n",
        "    ('to_dense', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\n",
        "    ('clf', eclf),\n",
        "    ])\n",
        "\n",
        "voting = voting.fit(X_train, y_train)\n",
        "predictions = voting.predict(X_test)\n",
        "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, predictions, average='macro')))\n",
        "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, predictions, average='macro')))\n",
        "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, predictions, average='macro')))\n",
        "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, predictions)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Precision:   0.69\n",
            "Recall:   0.67\n",
            "F1-measure:   0.67\n",
            "Accuracy:   0.67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CARAd-dVsIgc",
        "colab_type": "text"
      },
      "source": [
        "### Bagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zasH9pLqyb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKHRYfJpkpD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vect = CountVectorizer(tokenizer=stem_tokenizer, max_features=1000, stop_words=noise, min_df=0.01, max_df=0.5) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOjyYVwtmrPe",
        "colab_type": "code",
        "outputId": "763e2fae-1848-4e64-c469-d45b1404252a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "X = count_vect.fit_transform(X)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'veri', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaMc-rn1snTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf1 = RandomForestClassifier(n_estimators=50, random_state=1)  \n",
        "clf2 = GradientBoostingClassifier(n_estimators=50, random_state=1)\n",
        "clf3 = ExtraTreesClassifier(n_estimators=50, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p77X4-qvezC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bagging1 = BaggingClassifier(base_estimator=clf1, n_estimators=10, max_samples=0.8, max_features=0.8)\n",
        "bagging2 = BaggingClassifier(base_estimator=clf2, n_estimators=10, max_samples=0.8, max_features=0.8)\n",
        "bagging3 = BaggingClassifier(base_estimator=clf3, n_estimators=10, max_samples=0.8, max_features=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFWg4LOLvxKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label = ['RandomForestClassifier', 'GradientBoostingClassifier', 'ExtraTreesClassifier', 'Bagging RF', 'Bagging GradientBoosting', 'Bagging ExtraTrees']\n",
        "clf_list = [clf1, clf2, clf3, bagging1, bagging2, bagging3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-InlZtmrNg9",
        "colab_type": "code",
        "outputId": "8215b3c9-bddc-4f16-a29b-e83fffcd5bf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "for clf, label in zip(clf_list, label):      \n",
        "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
        "    print (\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.60 (+/- 0.01) [RandomForestClassifier]\n",
            "Accuracy: 0.62 (+/- 0.01) [GradientBoostingClassifier]\n",
            "Accuracy: 0.60 (+/- 0.01) [ExtraTreesClassifier]\n",
            "Accuracy: 0.63 (+/- 0.02) [Bagging RF]\n",
            "Accuracy: 0.62 (+/- 0.01) [Bagging GradientBoosting]\n",
            "Accuracy: 0.63 (+/- 0.01) [Bagging ExtraTrees]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji5x0jFyKOvB",
        "colab_type": "text"
      },
      "source": [
        "### Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBhp_dbytZjL",
        "colab_type": "code",
        "outputId": "0238e6d2-ed12-45cf-db59-6d4957783559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from mlxtend.classifier import StackingClassifier"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM7jkz2TdFvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf1 = RandomForestClassifier(n_estimators=50, random_state=1)  \n",
        "clf2 = SGDClassifier()\n",
        "clf3 = ExtraTreesClassifier(n_estimators=50, random_state=1)\n",
        "gradboost = GradientBoostingClassifier(n_estimators=50, random_state=1)\n",
        "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
        "                          meta_classifier=gradboost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INAd6DxDtMGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label = ['RandomForest', 'SGD', 'Extra Trees', 'Stacking Classifier']\n",
        "clf_list = [clf1, clf2, clf3, sclf]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iENdGDPeyQOO",
        "colab_type": "code",
        "outputId": "cfe14a88-fd52-498b-d57b-61b2dbfd6a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "clf_cv_mean = []\n",
        "clf_cv_std = []\n",
        "for clf, label in zip(clf_list, label):\n",
        "        \n",
        "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
        "    print (\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
        "    clf_cv_mean.append(scores.mean())\n",
        "    clf_cv_std.append(scores.std())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.60 (+/- 0.01) [RandomForest]\n",
            "Accuracy: 0.57 (+/- 0.02) [SGD]\n",
            "Accuracy: 0.60 (+/- 0.01) [Extra Trees]\n",
            "Accuracy: 0.61 (+/- 0.01) [Stacking Classifier]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}